{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12845203,"sourceType":"datasetVersion","datasetId":8124312}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch librosa scikit-learn soundfile transformers datasets numpy matplotlib pandas torchcodec ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-21T12:45:17.573303Z","iopub.execute_input":"2025-08-21T12:45:17.573622Z","iopub.status.idle":"2025-08-21T12:46:58.700886Z","shell.execute_reply.started":"2025-08-21T12:45:17.573598Z","shell.execute_reply":"2025-08-21T12:46:58.700188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoProcessor, MusicgenForConditionalGeneration\nfrom typing import Optional\n\n#version_1\n\nclass MusicgenWithResiduals:\n    def __init__(\n        self,\n        model_name: str = \"facebook/musicgen-small\",  # or \"facebook/musicgen-medium\", \"facebook/musicgen-large\"\n        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    ):\n        print(f\"Loading model {model_name} to {device}...\")\n        self.model = MusicgenForConditionalGeneration.from_pretrained(\n            model_name,\n            trust_remote_code=True,\n            output_hidden_states=True\n        ).to(device)\n\n        print(\"Loading processor...\")\n        self.processor = AutoProcessor.from_pretrained(model_name)\n        self.model.freeze_text_encoder()\n        self.model.freeze_audio_encoder()\n\n        self.device = device\n        self.hidden_states = {}\n\n        def hook_fn(module, input, output):\n            #print(\"Hook triggered!\")\n            if hasattr(output, \"hidden_states\"):\n                #print(\"Hidden states captured!\")\n                layer_names=[]\n                if hasattr(self.model, 'decoder') and hasattr(self.model.decoder.model.decoder, 'layers'):\n                    layer_names += [f\"decoder.layer.{i}\" for i in range(len(self.model.decoder.model.decoder.layers))]\n                #print(len(output.hidden_states))\n                self.hidden_states = {\n                    layer_names[i]: output.hidden_states[i+1]\n                    for i in range(len(layer_names))\n                }\n            else:\n                print(f\"Output structure: {type(output)} - {output}\")\n\n        self.model.decoder.model.decoder.register_forward_hook(hook_fn)\n        print(\"Model ready!\")\n\n    def generate_with_residuals(\n        self,\n        text: str = None,\n        audio: Optional[torch.Tensor] = None,\n        sampling_rate: int= None,\n        max_new_tokens: int = 10,\n        temperature=1e-3,\n        **kwargs\n    ):\n\n        self.model.decoder.config.output_hidden_states = True\n        inputs = {}\n\n        if text is None and audio is None:\n            inputs = self.model.get_unconditional_inputs(num_samples=1)\n        else:\n            inputs = self.processor(\n                text=text,\n                audio=audio,\n                sampling_rate=sampling_rate,\n                padding=True,\n                return_tensors=\"pt\"\n            ).to(self.device)\n\n        # Move inputs to device\n        #inputs = {k: v.to(self.device) for k, v in inputs.items()}\n\n        # Generate\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                output_hidden_states=True,\n                return_dict_in_generate=True,\n                max_new_tokens=max_new_tokens,\n            )\n\n        return {\n            \"audio_values\": outputs.sequences,\n            \"residual_streams\": self.hidden_states,  # Return the dictionary of hidden states\n            \"sampling_rate\": self.model.config.audio_encoder.sampling_rate\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T12:53:22.683948Z","iopub.execute_input":"2025-08-23T12:53:22.684444Z","iopub.status.idle":"2025-08-23T12:53:58.199782Z","shell.execute_reply.started":"2025-08-23T12:53:22.684414Z","shell.execute_reply":"2025-08-23T12:53:58.198437Z"}},"outputs":[{"name":"stderr","text":"2025-08-23 12:53:34.833312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755953615.136226      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755953615.248247      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# get the residuals from given input samples|\nimport numpy as np\nimport librosa\nfrom datasets import load_dataset\nimport re\nimport os\n\ndef sanitize_string(s):\n    return re.sub(r'[^a-zA-Z0-9]', '', s)\n\ndef process_with_residuals(model, data, count):\n    target_sr = 32000\n    #print(data)\n    audio = np.array(data['audio'][0]['array'])\n    sr = data['audio'][0]['sampling_rate']\n    genre = data['genre'][0]\n    prompt = f\"Generate {genre} music continuing the given audio\"\n    #print(audio['array'].shape)\n    audio = librosa.resample(y=audio, orig_sr=sr, target_sr=target_sr)\n    audio_segments = [audio[i*target_sr:(i+10)*target_sr] for i in range(0, int(audio.shape[0]//target_sr), 10)]\n    result = []\n    for segment in audio_segments:\n        #print(f\"Segment shape: {segment.shape} segment type: {type(segment)}\")\n        outputs = model.generate_with_residuals(\n            text=prompt,\n            audio=segment,\n            sampling_rate = target_sr,\n            max_new_tokens=512,\n            guidance_scale=3.0,\n            do_sample=True\n        )\n        print('generated outputs')\n        # Get the residual stream from the last layer\n        residual = np.array([outputs['residual_streams'][i].detach().cpu().numpy() for i in outputs['residual_streams']])\n        \n        # Create result dictionary with all original features and new data\n        result.append({\n            'genre': genre,\n            'generated_audio': outputs['audio_values'].detach().cpu().numpy(),\n            'residual_stream': residual,\n            'sampling_rate': outputs['sampling_rate'],\n            'prompt_used': prompt\n        })\n    \n    return result\n\ndef save_activations(\n    save_dir,\n    dataset,\n):\n    os.makedirs(save_dir, exist_ok=True)\n    print('Made folder')\n    idx = 0\n    for data in dataset.iter(batch_size = 1):\n        genre = data['genre'][0]\n        name = f\"{genre}_{idx}\"\n        processed_data = process_with_residuals(model, data, idx)\n        for result in range(len(processed_data)):\n            print(f\"processed {name}_{result}\")\n            # Save using numpy's save function\n            save_path = os.path.join(save_dir, f\"{name}_{result}.npz\")\n            np.savez(\n                save_path,\n                **processed_data[result]\n            )\n            \n            if idx % 1 == 0:  # Print progress every 10 items\n                print(f\"Processed {idx} samples\")\n        idx+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T12:51:51.357905Z","iopub.execute_input":"2025-08-21T12:51:51.358574Z","iopub.status.idle":"2025-08-21T12:51:51.366951Z","shell.execute_reply.started":"2025-08-21T12:51:51.358541Z","shell.execute_reply":"2025-08-21T12:51:51.366193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MusicgenWithResiduals()\nlewtun_modified = load_dataset(\"roovy54/lewtun_music_genres_modified\", streaming=True)\nsave_activations('lewtun', lewtun_modified['train'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%bash\ncd /kaggle/working/fma_medium\nzip -r /kaggle/working/output_lewtun.zip .\nrmdir lewtun","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-15T12:01:59.510569Z","iopub.execute_input":"2025-08-15T12:01:59.511386Z","iopub.status.idle":"2025-08-15T12:01:59.984787Z","shell.execute_reply.started":"2025-08-15T12:01:59.511352Z","shell.execute_reply":"2025-08-15T12:01:59.984074Z"}},"outputs":[],"execution_count":null}]}