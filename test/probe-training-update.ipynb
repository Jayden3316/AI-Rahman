{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gdown","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!gdown --id 1VeWJvKGgg8sMMc0o_zmrSzgDCdTccGfQ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip results.zip -d /kaggle/working/results","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!rm results.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:42:09.419846Z","iopub.execute_input":"2025-08-27T22:42:09.420132Z","iopub.status.idle":"2025-08-27T22:42:10.494174Z","shell.execute_reply.started":"2025-08-27T22:42:09.420096Z","shell.execute_reply":"2025-08-27T22:42:10.492810Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef get_data(save_dir):\n    data = []\n    # Iterate through each file in the directory\n    for filename in os.listdir(save_dir):\n        if filename.endswith('.npz'):\n\n            # Load the file\n            loaded_file = dict(np.load(os.path.join(save_dir, filename), allow_pickle=True))\n\n            # Reshape the residual streams\n            if not data:\n                print(\"Residual shape:\", loaded_file['residual_stream'].shape)\n            residual_unconditional = loaded_file['residual_stream'].reshape(24, 2, 1024)[:, 1, :]\n            residual_conditional = loaded_file['residual_stream'].reshape(24, 2, 1024)[:, 0, :]\n\n            # Create a dictionary for the current file's data\n            file_data = {\n                'filename': filename,\n                'genre': loaded_file['genre'],  # Keeping original genre if needed\n            }\n\n            # Add residuals to the dictionary\n            for layer in range(24):\n                file_data[f'residual_conditional_{layer + 1}'] = residual_conditional[layer, :]\n                file_data[f'residual_unconditional_{layer + 1}'] = residual_unconditional[layer, :]\n\n            # Append the file data to the list\n            data.append(file_data)\n\n    return data\n\n# Define your linear probe (a single linear layer without bias)\nclass SimpleNN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(SimpleNN, self).__init__()\n        self.linear = nn.Linear(input_size, num_classes, bias=False)  # No bias\n\n    def forward(self, x):\n        return self.linear(x)  # Raw dot products\n\ndef sign(x):\n    return torch.where(x < 0, -1, 1)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing processed chord files\nsave_dir = '/kaggle/working/results/lewtun/'\ndata = get_data(save_dir)\ndf = pd.DataFrame(data)\ninv_genre_map = {'Classical': 0, 'Electronic': 1, 'Rock': 2, 'Jazz': 3}\ndf['label'] = df['genre'].map(lambda x: inv_genre_map[x])\nmin_count = df.groupby('label').size().min()\ncdf = df.groupby('label').sample(n=min_count, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlayers = list(range(1, 25))  # Layers 1 to 24\nacc_list = []\nloss_list = []\nvloss_list = []\nweights_dict = {i:0 for i in range(1, len(layers)+1)}\nprint(weights_dict)\n\ncond_col = f\"residual_conditional_{i}\"\nuncond_col = f\"residual_unconditional_{i}\"\nnum_classes = 4  # For example, 10 classes\ninput_size = 1024\n\nfor i in range(1, len(layers)+1):\n    X_cond = normalize(np.stack(cdf[cond_col].values), norm='l2')  # Shape: (N, 1024)\n    y = cdf[\"label\"].values.astype(np.float32)  # Convert labels to float for MSELoss\n\n    # Split data into training and testing sets\n    X_cond_train, X_cond_test, y_cond_train, y_cond_test = train_test_split(\n        X_cond, y, test_size=0.15, random_state=42, shuffle=True, stratify=y\n    )\n\n    # Convert features and labels to PyTorch tensors\n    X_cond_train = torch.tensor(X_cond_train, dtype=torch.float32).to(device)\n    X_cond_test  = torch.tensor(X_cond_test, dtype=torch.float32).to(device)\n    y_cond_train = torch.tensor(y_cond_train, dtype=torch.float32).to(device)\n    y_cond_test  = torch.tensor(y_cond_test, dtype=torch.float32).to(device)\n\n    # Ensure labels have the correct shape (N, 1) since MSELoss expects predictions to match the shape of targets\n    y_cond_train = y_cond_train.unsqueeze(1)\n    y_cond_test = y_cond_test.unsqueeze(1)\n\n    # Define mini-batch size and create DataLoaders\n    batch_size = 1024\n\n    def get_loader(X, y, batch_size):\n        dataset = TensorDataset(X, y)\n        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n    # Create loaders for one of the conditions (repeat similarly for the other)\n    cond_train_loader = get_loader(X_cond_train, y_cond_train, batch_size)\n    cond_test_loader  = get_loader(X_cond_test, y_cond_test, batch_size)\n\n    # Initialize the model, optimizer, and MSE loss.\n    model = SimpleNN(input_size=input_size, num_classes=num_classes).to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.005)\n    loss_fn = nn.MSELoss()\n\n    num_epochs = 250\n\n    # Training loop using MSELoss with one-hot targets\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        for X_batch, y_batch in cond_train_loader:\n            optimizer.zero_grad()\n            outputs = model(X_batch)  # Raw outputs: [batch_size, num_classes]\n            # Optionally, you could apply a softmax if you want the outputs to behave like probabilities:\n            # outputs = torch.softmax(outputs, dim=1)\n            loss = loss_fn(outputs, y_batch)  # Compare to one-hot target\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * X_batch.size(0)\n        if (epoch + 1) % 10== 0:\n            epoch_loss = running_loss / len(cond_train_loader.dataset)\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.8f}\")\n    loss_list.append(epoch_loss) #Loss at the end of training loop\n    # Evaluation: compute validation loss and accuracy\n    model.eval()\n    correct = 0\n    total = 0\n    val_loss = 0.0\n\n    with torch.no_grad():\n        for X_batch, y_batch in cond_test_loader:\n            outputs = model(X_batch)\n            # Compute the batch loss (MSELoss)\n            loss = loss_fn(outputs, y_batch)\n            # Multiply by the batch size to accumulate the total loss\n            val_loss += loss.item() * X_batch.size(0)\n\n            # Apply sign function to threshold the outputs to -1 or 1\n            predicted = sign(outputs)\n            total += y_batch.size(0)\n            correct += torch.eq(predicted, y_batch).sum().item()\n            # Optional: print the actual and predicted values for each batch\n            #print(\"Actual:\", y_batch.cpu().numpy().squeeze(), \"Predicted:\", predicted.cpu().numpy().squeeze())\n\n    # Calculate average loss and accuracy over the validation set\n    val_loss /= total\n    accuracy = correct / total\n    print(\"--- Layer: \", i, \"---\")\n    print(f\"Accuracy (Conditional): {accuracy:.8f}\")\n    print(f\"Validation Loss (Conditional): {val_loss:.8f}\")\n\n    acc_list.append(accuracy)\n    vloss_list.append(val_loss)\n    # Store the full weight matrix instead of flattening\n    weights_dict[i] = model.linear.weight.detach().cpu().numpy()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nweights_array = np.array([weights_dict[i+1] for i in range(len(weights_dict))])\nprint(weights_array.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:58:32.967276Z","iopub.execute_input":"2025-08-27T22:58:32.967575Z","iopub.status.idle":"2025-08-27T22:58:32.972914Z","shell.execute_reply.started":"2025-08-27T22:58:32.967551Z","shell.execute_reply":"2025-08-27T22:58:32.972327Z"}},"outputs":[{"name":"stdout","text":"(24, 4, 1024)\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"np.save('weights.npy', weights_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-27T22:59:35.322791Z","iopub.execute_input":"2025-08-27T22:59:35.323139Z","iopub.status.idle":"2025-08-27T22:59:35.328074Z","shell.execute_reply.started":"2025-08-27T22:59:35.323115Z","shell.execute_reply":"2025-08-27T22:59:35.327288Z"}},"outputs":[],"execution_count":28}]}